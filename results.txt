Ich begrüße Sie recht herzlich zur Vorlesung Rechnerstrukturen. In diesem Wintersemester ist es die erste Vorlesung und wir müssen uns leider online treffen, weil eben Corona sich nicht groß verbessert hat und im Hörsaal es schwierig wäre, dass wir uns alle treffen. Ich würde vorschlagen, wir gehen erst mal das Prozedere durch und dann sprechen wir über Inhalte. Wenn Sie mich jetzt hier live hören, dann haben Sie es ja geschafft unsere Vorlesungsseite zu finden. Ich blende die hier noch mal kurz ein. Wir haben also für Sie eine Webseite vorbereitet, wo Sie den Stream sehen. Das ist bei mir jetzt der schwarze Bereich, weil ich den nicht noch mal mitlaufen lasse und Sie haben auf dieser Webseite im Wesentlichen zwei Möglichkeiten mit mir zu interagieren. Sie haben die Möglichkeit hier rechts eine Frage zu stellen. Diese Frage, die wird hier komplett anonym gestellt und wir haben hier einen Tutor mit in der Vorlesung, der versucht dann diese Frage, wenn es eine kurze Frage ist, möglichst sofort zu beantworten, textuell. Wenn es eine komplexere Frage ist, die sich besser per Sprache beantworten lässt, dann wird er diese Frage zu mir durchstellen und aufgrund dieses Streams gibt es zwar eine kleine Zeitverzögerung, aber ich werde natürlich dann versuchen, diese Frage hier live in der Vorlesung direkt zu beantworten. Sie haben hier unten die Möglichkeit zu chatten. Dieser Chat ist nicht ganz anonym bzw. ist nicht anonym, weil Sie sich da vorher bei StudIP oder über StudIP bei diesem Matrix Server anmelden müssen und dann sind Sie natürlich hier mit Ihrer entsprechenden ID unterwegs und wir können mitverfolgen, was gechattet wird. Also Fragen anonym, Chat nicht anonym und hier findet natürlich der Stream der Vorlesung statt. Okay, gut, dann können wir zur Vorlesung selbst kommen. Auch hier wird es jetzt erstmal ein paar organisatorische Hinweise geben und zwar, klar, Vorlesung wird gehalten von mir und die Übung wird gehalten von Florian Rommel. Es ist eine Frontalübung, also eine Hörsaalübung und wir werden uns abwechseln mit Vorlesung und Übung und zwar im Mittel wird es so sein, dass es immer zwei Vorlesungen gibt und dann kommt eine Übung. Das heißt, die Termine sind dienstags und freitags und je nachdem, also die ersten beiden Vorlesungen werde ich halten und ob dann Übung oder eine Vorlesung kommt, das entnehmen Sie auf jeden Fall immer entweder dem StudIP oder der Seite auf unseren Webseiten. Wir versuchen immer alle Informationen auch in StudIP zur Verfügung zu stellen. Das heißt, Sie können sich zwar alternativ auf unseren Webseiten und in StudIP informieren, aber alle wesentlichen Informationen stehen jeweils in StudIP. So auch eben der Ort der Vorlesung, die findet ja hier online statt und wir haben hier nochmal die Internetadresse, damit Sie diese Seite finden. Gut, alle die jetzt zuhören, die haben ja die Seite offensichtlich schon gefunden. Sie können natürlich alle Materialien zu dieser Vorlesung auch in StudIP herunterladen. Das heißt, es gibt Folien, es gibt Informationen. Sie können das ja inzwischen mit dem StudIP einfach immer nachschauen und sich auf den Stand halten. Ich habe die Folien im PowerPoint-Format und im PDF-Format zur Verfügung gestellt und kleiner Hinweis noch, ich bin bei der Vorbereitung der jeweiligen Vorlesung immer dabei, so ein bisschen aktuelles Material einfließen zu lassen. So auch diesmal und deshalb kann es sein, dass sich im Lauf des Semesters die eine oder andere Folie nochmal ändert. Also ich habe schon mal alle Folien hochgeladen, aber die endgültige Fassung der Folien steht erst am Semesterende zur Verfügung, weil ich eben während des Semesters noch Folien ändere und dann die Änderungen aber auch immer gleich hochlade. Sie finden dann natürlich auch in StudIP die Termine, falls Vorlesungen mal ausfallen sollten und Verschiebungen. Die Prüfung zur Vorlesung wird ganz normal stattfinden als Klausur, so wie Sie es gewöhnt sind und auch hier finden Sie alle Details in StudIP. Zum Beispiel werden wir ein Repetitorium am Ende des Semesters veranstalten und Termine und Ähnliches dann eben entsprechend in StudIP. Gut, worum geht es jetzt inhaltlich in dieser Vorlesung? Wir setzen ja zum Hören dieser Vorlesung voraus, dass Sie schon die Vorlesung Grundlagen der Rechnerarchitektur besucht haben. Was ist passiert in Grundlagen der Rechnerarchitektur? Ja gut, da haben wir versucht Ihnen zu erklären, wie so ein Prozessor denn funktioniert und wir haben begonnen mit ein paar Voraussetzungen in der Informationstheorie und haben dann uns schrittweise so einen Rechner gebaut, indem wir eben Execution Unit, Control Unit, Speicher und das Zusammenspiel derselben besprochen haben. Wir haben dann schon ein bisschen an Performance gedacht und einführend mal über Pipeline gesprochen und über Performance gesprochen und haben damit die Grundlagen gelegt, die Sie eigentlich schon brauchen, um jetzt dann diese Vorlesung vollständig zu verstehen. Hier jetzt in der Vorlesung Rechnerstruktur, worum geht es da? Wir wollen uns erst mal die Ziele einer Rechnerarchitektur anschauen. Also welche Vorgaben, welche Ziele habe ich, wenn ich Rechnerarchitektur betreibe? Es geht natürlich sehr schnell dann um Performance, also um Leistung und Möglichkeiten zur Leistungssteigerung und wir werden sehen, dass jedes einzelne Detail sehr wichtig ist in Bezug auf die Leistung. Also muss man erst mal Gedanken machen, wie soll denn mein Befehlssatz aussehen? Also welche Befehle erlaube ich? Wie konstruiere ich einen Befehlssatz? Was sind die Eigenschaften des Befehlssatzes? Was sind wichtige Eigenschaften? Da geht es dann im Kapitel Befehlssatzdesign drüber. Dann haben wir das Kapitel Alu-Design, da geht es ein bisschen dedizierter darum, wie man eine Alu-Performance, also geeignet für gute Performance designt. Datenpfad, da geht es natürlich sehr intensiv darum um Pipelining. Welche Stufen kann ich alles definieren? Wie arbeiten diese Stufen zusammen? Und das ist quasi eine direkte Fortsetzung der Pipeline-Grundlagen aus der Grundlagen der Rechnerarchitektur. Wir werden uns den Speicherzugriff ein bisschen genauer betrachten müssen, und zwar Caches. Also wie groß mache ich einen Cache? Warum ist die Cache-Hierarchie, wie sie heutzutage aussieht? Das wollen wir uns alles genauer anschauen. Und dann geht es schon ein bisschen in die Parallelität. Wir sprechen über Superskalarität, also das ist Parallelität bei Monoprozessoren. Und diese gestrichelte Linie, die zeigt dann den Übergang zum zweiten Teil der Vorlesung. Da geht es dann um parallele Rechnerarchitekturen. Und auch hier wollen wir uns natürlich das Thema Performance genauer anschauen, wie man auch Performance möglicherweise modellieren kann. Und dann sprechen wir über Multicore und Multithreading. Und am Schluss dann über Synchronisation. Gut, begleitende Literatur gibt es natürlich auch zu dieser Vorlesung. Und zwar, ja, das Wichtigste steht gleich an erster Stelle da oben. Das ist Computer Architecture, A Quantitative Approach. Das ist ein Buch von John Hennessy und David Patterson. Und ja, sagen wir mal so, es ist die Bibel des Rechnerarchitekten. Also wir haben daraus einige Beispiele entnommen. Und ein Teil der Vorlesung basiert auch ein bisschen auf diesem Buch. Also da steht eigentlich alles drin, was für einen Rechnerarchitekten so wichtig ist. Da gibt es dann auch noch ein bisschen eine Fortführung zu diesem Buch. Das heißt dann Computer Organization and Design. Übenfalls von David Patterson und John Hennessy. Ein richtig dicker Welser. Sie sehen ja, das erste Buch hat 708 Seiten, das hat 912 Seiten. Also auch da nochmal wichtige Informationen für Rechnerarchitekten. Zum Thema Mikrocontroller und Mikroprozessoren. Gibt es auch noch eine Buchempfehlung, wo man sich dann eben mal Details zur grundsätzlichen Funktionsweise von Mikroprozessoren nochmal holen kann. Gut, worum geht es jetzt genau bei der Rechnerarchitektur? Ja, wir haben jetzt schon mal ganz grob über Stichpunkte gesprochen. Aber was jetzt so die Ziele in der Rechnerarchitektur sind, darüber müssen wir zuerst sprechen, um dann auch ein bisschen genauer zu verstehen, was ist denn eigentlich jetzt genau Rechnerarchitektur? Jeder kann sich unter Architektur was vorstellen, wenn es um den Bau geht, wenn es um Häuser geht. Aber was sind denn die Aufgaben des Rechnerarchitekten, ist möglicherweise noch nicht so ganz klar. Und deshalb wollen wir uns erstmal anschauen, welche Ziele, welche Vorgaben habe ich denn eigentlich als Rechnerarchitekt? So, dafür brauchen wir dann natürlich das entsprechende nächste Kapitel. Dieses Kapitel ist überschrieben mit dem Thema Ziele der Rechnerarchitektur. Als Voraussetzungen, um dieses Kapitel zu verstehen, müssen Sie wie bei allen anderen Kapiteln dann auch natürlich die Grundlagen der Rechnerarchitektur gehört haben, weil wir einfach Begriffe verwenden, die da einführend definiert werden. Also da müssen Sie sich was darunter vorstellen können, sonst verstehen Sie natürlich einige Sachen nicht, die ich dann hier erkläre. Worum geht es? Wir möchten als Ziel für dieses Kapitel ausgeben, dass wir eine Motivation geben für eine performancebasierte, quantitative Sicht der Rechnerarchitektur. Also es geht um Leistung, es geht um Leistungssteigerung, um die S-Kurve, was diese S-Kurve bedeutet für den Rechnerarchitekten, was sie beschreibt, um Limits, die auch für uns als Rechnerarchitekten gelten. Also wir können natürlich die Physik nicht außer Kraft setzen. Wir sind an die Gesetze der Physik natürlich gebunden. Welche Optionen haben wir als Entwickler? Und dann unter diesen Vorkenntnissen nochmal betrachten die Inhalte der Vorlesung Rechnerstruktur. Okay, ja, Leistung, Performance, was ist das eigentlich? Das kennen wir schon aus Schulzeiten, gibt es eine einfache Formel, die Leistung ist definiert als Arbeit durch Zeit. Und ganz allgemein gesprochen gilt natürlich diese Formel auch in der Rechnerarchitektur. Also da sind wir erstmal im einfachen Ansatz für die Definition der Leistung, unterscheiden wir uns als Rechnerarchitekten überhaupt nicht von anderen Disziplinen. Ja, wie wird bei uns, also in der Rechnerarchitektur, wie wird hier die Leistung bestimmt? Was sind die wichtigsten Parameter für die Leistung? Gut, es geht natürlich hauptsächlich hier um Technologie erstmal. Also wenn Sie überlegen, grundlagend in der Rechnerarchitektur haben wir das besprochen, so die ersten Rechner waren mechanisch, dann kamen elektrische, elektronische Rechner dazu, das heißt es wurde viel mit Relays, mit Röhren gearbeitet, dann wurden irgendwann die Transistoren erfunden und die integrierten Schaltkreise, also ICs erfunden und da gibt es natürlich unterschiedliche Schaltkreistechnologien, etwas schnellere, etwas langsamere, also zum Beispiel ECL-Technologie sind bipolare Transistoren, das ist natürlich geeignet für Hochleistungsschaltungen, allerdings haben die den Nachteil, dass sie fertigungstechnisch ein bisschen schwieriger sind, weil sie auch viel, viel mehr Hitze erzeugen als die einfachen CMOS-Transistoren und da haben wir dann schon den ersten Entscheidungsspielraum, welche Schaltkreistechnologie setze ich ein, ist also an einer bestimmten Stelle Leistung ganz besonders wichtig, dann gehe ich möglicherweise zu diesen ECL-Transistoren, obwohl die eben fertigungstechnisch einfach ein bisschen schwieriger zu handeln sind, es muss sich also lohnen, wenn ich diese Technologie einsetze. Gut, es ist natürlich möglich, dass es in Zukunft auch noch ganz andere Technologien gibt, also Sie haben wahrscheinlich alle schon mal das Schlagwort Quantencomputing gehört, da kann es dann mal ganz anders ausschauen und es ist auch noch nicht so ganz klar, wie so ein Quantencomputer dann hardware-technisch überhaupt aufgebaut ist, also welche Technologie da dann verwendet wird. Gut, die Architektur, die wird eben definiert durch Struktur und Verhalten und diese Architektur, die wird eben auch als Design bezeichnet, also man könnte auch Rechner-Design praktisch sagen und darum geht es in dieser Vorlesung. Es ist immer wichtig, wenn man ein bisschen in die Vergangenheit schaut und sieht, wie hat sich denn das Ganze überhaupt entwickelt und wir wollen jetzt natürlich nicht im Mittelalter bei Leibniz wieder anfangen, sondern wir möchten ein bisschen später anfangen, nämlich im Jahr 1965 und betrachten mal die Zeitskala bis 1990. Das war nämlich definitiv die Zeit von Single- Prozessoren und man sieht, begonnen hat das Ganze mit Minicomputern und Mainframes und die Unterscheidung jetzt zwischen Supercomputer, Mainframe und Minicomputer und Workstation, was ist die eigentlich? Also Supercomputer ist normalerweise ein Rechner, wo es weltweit ganz wenig Installationen gibt, die aber eben die leistungsstärksten Rechner weltweit sind, also sehr teuer, sehr leistungsstark in der jeweiligen Zeitphase natürlich. Mainframes, das sind Großrechner, wie sie in der Industrie, in der öffentlichen Verwaltung eingesetzt werden, also das sind teure Rechner, die relativ leistungsstark sind, die aber keinesfalls sich der Otto Normalverbraucher leisten kann. Da braucht man schon eine große Institution, die den entsprechenden finanziellen Background hat, um so einen Teil zu betreiben und es muss dann auch richtig arbeiten, um dieses Geld wieder reinzuholen. Minicomputer, das waren so kleinere Rechner, also irgendwas zwischen Mainframe und Workstation, die waren eben relativ früh auch schon vernetzt, so kleinere Institutionen, also wie zum Beispiel ein Fachgebiet an der Uni, konnte sich so ein Teil schon mal leisten, aber das war eben nichts, was man sich zu Hause ins Wohnzimmer stellt. Und Workstations, wenn am Anfang auch schon noch relativ teuer, das waren dann so die ersten Personalcomputer, das heißt, die konnte man sich dann zur Not auch zu Hause privat leisten, auch wenn sie eben am Anfang sehr, sehr teuer war. Und was wir hier sehen, ist die Leistungssteigerung, die die erfahren haben und wir sehen natürlich insgesamt auf der linken Skala, auf der Leistungsskala, wieder eine exponentielle Skala und ja, in Corona-Zeiten ist uns ja alles, wissen wir alle, was exponentielles Wachstum bedeutet, also sehr schnelles Wachstum und was aber wichtig ist, ist wie steil die einzelnen Geraden sind und da sehen wir schon, dass die Workstationsgerade, also die Mikroprozessorgerade, dass die eigentlich den steilsten Anstieg hat und wir werden dann gleich in der nächsten oder in der übernächsten Folie sehen, dass natürlich die dann andere geschnitten hat, andere Geraden und bzw. dazu geführt hat, dass es halt immer weniger Markt für Minicomputer gab, gibt es eigentlich gar nicht mehr Mainframes, ja bestehen heutzutage auch im Wesentlichen aus Mikroprozessoren, möglicherweise mehreren. Insgesamt kann man sagen, man hat eine Leistungssteigerung von irgendwas zwischen 60 und 100 Prozent pro Jahr, also eben exponentielles Wachstum. Gut, ganz oben sehen wir noch Parallel Computing, da steigen wir ein bisschen später ein in der Skala, so ungefähr bei 1980, 85, da geht es dann los mit Parallel Computing und das bedeutet natürlich, dass man mehrere Rechner zusammenschaltet, damit die gemeinsam ein großes Problem lösen. Wie haben sich die Mikroprozessoren über die Zeit entwickelt? Also schauen wir mal die Zeitskala von 1970 bis 2005 an, da hat ein Herr Moore im Jahr 1965 ein Gesetz aufgestellt, das heutzutage immer noch gilt, für exponentielles Wachstum ist es natürlich sehr sehr lang von 1965 bis heute, also das sind über 50 Jahre und er hat vorhergesagt, dass die Anzahl von Transistoren sich ungefähr alle zwei Jahre verdoppeln wird. Diese Vorhersage bezieht sich natürlich auf die Entwicklung einzelner Mikroprozessoren, wenn ich natürlich viele Mikroprozessoren zu einem Multiprozessor zusammenschalte, dann kann ich da natürlich ein noch stärkeres Wachstum erreichen und dann, wenn man sich die Systeme anschaut, dann kann man davon sprechen, dass sich das ungefähr alle Jahre verdoppelt hat. Das Architekturziel ganz klar bei den Mikroprozessoren war der Leistungswunsch, man will immer mehr Leistung, immer schneller alles machen. Also viele Sachen, ich kann mich erinnern an die 90er Jahre, wo man gesagt hat, okay, um Real-Time 3D Grafik zu machen, da braucht man einen Supercomputer, also eine mehrere Millionen Euro schwere Installation, das hat sich heute alles erledigt, sowas macht heutzutage eine Grafikkarte für ein paar hundert Euro. Die große Frage ist, gilt das Gesetz von Moore immer noch und wie lange soll es denn gelten? Und da kann man jetzt dann einfach nochmal anschauen, wie die tatsächliche Entwicklung ist, wenn wir uns jetzt mal die untere Grafik anschauen, da haben wir eine Zeitskala von 1970 bis 2010 und da geht es wirklich um die Anzahl der Transistoren auf dem Die, also die Anzahl der Transistoren pro Chip und da sehen wir, dass das eben von ein paar tausend gewachsen ist bis zum Jahr 2007, 2008 auf eine Milliarde oder mehr als eine Milliarde Transistoren auf dem Chip, also exponentielles Wachstum und wenn man das auf heute jetzt hochrechnet, gilt dieses Wachstum derzeit immer noch, wenn man sich die neuesten Prozessoren anschaut, wie viele Transistoren da drauf sind. Was ist in dieser Zeit mit der Performance passiert? Man hat auf der Grafik hier, da sehen Sie die Zeitskala von 1978 bis 2006 ungefähr und da hat man hier mal die Leistung aufgezeichnet und zwar die Leistung normiert oder relativ zu einer VEX. VEX war damals die Standardmaschine, die fast jede Uni mindestens eine hatte davon und im Vergleich dazu die Leistung der ausgelieferten Systeme von IBM, von Apple, von anderen Firmen, da sieht man, dass eben diese Leistung entsprechend auch gewachsen ist, also da so mit 64-Bit-Prozessoren

Allerdings, was hier auffällt, ist, dass es hier einen steilen Anstieg gegeben hat, so ungefähr von 1986 bis ungefähr 2002, also ein Wachstum von 52% pro Jahr, und dass dann diese Performance aber ein bisschen nachgelassen hat, also immer noch ein gutes Wachstum da, aber doch so ein deutlicher Knick in dieser Kurve zu sehen ist. Man kann das ein bisschen festmachen an der Entwicklung der Taktfrequenz. Man hat ja relativ lange gedacht, dass es kein Problem ist, die Taktfrequenz immer weiter nach oben zu schrauben. Die Roadmaps von Intel haben dann irgendwann mal prognostiziert, dass es so bis 10 Gigahertz weiter geht mit dieser Taktfrequenz, und was man dann aber erfahren musste, das sehen wir hier so im Jahr 2002 ungefähr, dass das Ganze dann bei 3 Gigahertz stagniert ist, diese Erhöhung der Taktfrequenz. So, was ist passiert? Ja, kann man ganz einfach ausdrücken, die Chips sind einfach zu heiß geworden und man hat keine Möglichkeit gefunden, diese Wärme vernünftig abzuführen und wenn man diese Wärme nicht abführen kann, dann führt es halt dazu, dass die Chips so heiß werden, dass sie dann schmelzen und dann war es das mit den Chips. Also haben wir hier eben einen deutlichen Einbruch erlebt in der Wachstumsrate der Taktfrequenz. Ja, wie schaut es bei den Multiprozessoren aus? Hier schaut es ein bisschen besser aus. Hier sieht man auch diesen Knick aus dem... Ja, gerne. Kann ich die lesen? Ok, also wir hatten hier jetzt gerade eine Frage und zwar, wie unterscheiden sich denn eigentlich die Supercomputer und Minicomputer in der Technik? Diese Frage ist ganz leicht zu beantworten. Also früher war das ein Riesenunterschied, weil man natürlich bei den Supercomputern immer auch die neueste Technologie verwendet hat, also ganz andere Schaltkreise als man bei normalen Rechnern verwendet hat. Heutzutage ist es so, dass der Unterschied von den Supercomputern zu normalen Rechnern, dass der eigentlich marginal ist. Das Einzige was man macht, man hat ein sehr schnelles Verbindungssystem und ansonsten verwendet man meistens Components of the shelf, das heißt irgendwelche Komponenten aus dem Regal. Also man sucht sich sehr schnelle Mikroprozessoren, man sucht sich richtig dicke, sehr schnelle Grafikkarten. Dann versucht man erstmal vernünftige Boards aufzubauen. Also auf einem Board sind zwei bis vier Prozessoren mit zwei bis vier dicken Grafikkarten und dann verbindet man diese Boards mit sehr schnellen Interconnects und baut da draußen Rack und dann verbindet man halt nochmal diese Racks mit sehr schnellen Interconnects. Also man baut einfach Systeme aus tausenden Prozessoren auf, wobei meistens diese Prozessoren Components of the shelf sind. Das heißt heutzutage besteht in der Basistechnologie kein Unterschied mehr zwischen dem PC den sie zu Hause unterm Schreibtisch haben und dem Mikroprozessor der eingebaut ist in so einem Supercomputer. Und dementsprechend war eben auch dieser Knick, wo wir gerade drüber gesprochen haben, im Jahr 2002, war nicht vorhanden, weil ich habe zwar einen Knick in der Taktdraht, dass die nicht weiter nach oben geht, aber was ich natürlich machen kann, ist einfach dann mehr Prozessoren nehmen und zu einem großen Rechner zusammenschalten. Und was wir hier sehen ist eine Grafik aus der top500.org Das ist eine herstellerübergreifende Plattform, wo alle Ergebnisse zu Supercomputern irgendwie zu finden sind, also Leistungsergebnisse zu finden sind und diese Grafik verdeutlicht praktisch die Leistungsentwicklung bei den Superrechnern oder bei den Multiprozessoren. Und zwar ist das Ganze so zu lesen, dass diese grüne Linie hier, die zeigt praktisch die Summe der Leistung von den 500 weltweit schnellsten Rechnern. Diese braune Dreieckslinie hier, die zeigt die Leistung des jeweils zu dem Zeitpunkt schnellsten Rechners weltweit und diese blaue Linie, das zeigt die durchschnittliche Leistung aller Rechner, die in den top500 gelistet waren, also der 500 schnellsten Rechner weltweit. Und hier sehen wir ebenso im Jahr 2002 keinen Einbruch, sondern wir haben wieder eine exponentielle Skala, also exponentielles Wachstum hier auf der Geraden und das läuft munter weiter. Das heißt, auch heute ist der Leistungshunger noch ungebremst, auch wenn ich inzwischen so ein kleines Kraftwerk brauche, um solche Rechner überhaupt betreiben zu können. Gut, jetzt gibt es da natürlich einige Fragestellungen dazu, und zwar, wie messe ich denn überhaupt Leistung? Was ist denn die richtige Metrik, die ich verwenden sollte? Ist denn MIPS, also Megainstruktionen pro Sekunde, aussagekräftig oder ist FLOPS, also Floating Point Operationen pro Sekunde aussagekräftig? Oder muss ich denn Leistung möglicherweise anders messen? Es ist auch gut, wenn man sich jetzt genauer anschaut, woher kommt denn diese Leistungsexplosion? Also wer hat denn die entscheidenden Beiträge geleistet? War das die Fertigungstechnologie, die Weiterentwicklung? Waren es die Rechner-Architekten, also haben die tolle Ideen gehabt bei der Struktur dieser ganzen Systeme? Woher kommt also diese Leistungssteigerung, um dann in Zukunft die Systeme eben noch besser zu machen? Warum erleben wir momentan schon eine Sicht- und spürbare Sättigung, zumindest im Bereich der Mikroprozessoren? Da hat sich ja, was jetzt die Leistung einzelner Prozessoren betrifft, in den letzten 10 Jahren nicht so viel getan. Man ist hauptsächlich durch Parallelisierung noch leistungsstärker geworden, der Einzelprozessor, Einzelmikroprozessor hat sich nicht so großartig verbessert mehr. Wo habe ich Leistungsreserven? Also der Leistungshunger ist immer noch ungebremst und natürlich werde ich in Zukunft noch schnellere Rechner brauchen. Wie kann ich das erreichen? Und dann eben muss ich einen besonderen Blick auf die Metriken werfen. Zum Beispiel, wenn ich jetzt aufs Smartphone schaue, dann ist mir ein Energieverbrauch vielleicht wichtiger als Leistung. Natürlich möchte ich auch mit dem Smartphone irgendein Game spielen und brauche da mal Grafikleistung, aber wenn ich das Smartphone zum Beispiel in der Tasche habe und nichts mitmache, dann sollte es natürlich auch möglichst wenig Strom brauchen und vielleicht kommen wir ja mal wieder dazu, dass so ein Handy 2-3 Tage ohne Stromnachfuhr auskommt. Also ich erinnere mich an mein gutes altes Nokia, das konnte ich auch mal 5-6 Tage mit mir rumtragen, ohne dass ich dann ein Ladegerät dafür gebraucht habe. Dafür muss ich dann betrachten zum Beispiel Energieverbrauch pro Chipfläche oder andere Metriken, Energieverbrauch pro Instruktion oder Operation. Dann muss ich mir anschauen, möglicherweise abschaltbare Chipreale und Chipreale mit Areale mit unterschiedlichen Taktfrequenzen. Das heißt, ich heize nicht immer gleichzeitig die ganze Chipfläche auf, sondern ich habe bestimmte Bereiche, die bestimmte Funktionen haben und die können schneller oder langsamer getaktet werden oder im Idealfall, wenn sie nicht benötigt werden, sogar komplett abgeschaltet werden. Das sind also interessante Fragestellungen für die Zukunft, die wir hier haben. Gut, schauen wir uns nochmal die Performanceformel an, weil anhand der Performanceformel kann man natürlich sehr schnell sich Gedanken machen, wo es sich lohnt, noch Verbesserungen durchzuführen und welche Verbesserungen eben leistungssteigernd wirken. Also wir haben in Grundlagen der Rechenarchitektur diese Performanceformel mal aufgestellt. Das heißt, wir haben hier oben im Zähler stehende Taktfrequenz, im Nenner stehen haben wir unsere Cycles per Instruction, also wie viele Zyklen brauche ich, um eine Instruktion im Mittel abzuarbeiten und wir haben hier ein Memory Delay. Wenn ich Verstärkungen erfahre beim Zugriff auf den Speicher, dann wirkt sich das natürlich leistungsmindernd aus. Wir haben hier so einen Referenzfaktor, der die Stärke unseres Befehlsatzes beschreibt und wir haben so einen Parallelitätsfaktor, der multiplikativ eingeht, wenn ich mit vielen oder mehreren Prozessoren arbeite. Gut, wenn ich also jetzt den Einzelprozessor mir anschaue, welche Möglichkeiten habe ich, Leistung zu steigern, ganz einfach, alles was im Zähler steht, nach oben, was im Nenner steht, nach unten. Das heißt, ich kann versuchen, die Taktfrequenz zu steigern, aber da wissen wir jetzt schon, dass wir da technologische Probleme haben, über die 4 GHz hinaus zu gehen. Ich kann dann mein CPI vermindern, also je weniger Zyklen pro Instruktion ich brauche, desto schneller. Und ich kann natürlich versuchen, einen relativ starken Befehlsatz zu machen. Nicht zuletzt hat die Sättigung der letzten Jahre wieder die Diskussionen über die VL-ISW-Maschine aufgebracht, also Very Large Instruction Word Maschine. Das heißt, dass ich versuche, in einem Befehl möglichst viel Arbeit zu leisten. Das geht hier multiplikativ ein, kann natürlich auch zur Beschleunigung führen. Da spielt natürlich die Anzahl der Prozessoren eine entscheidende Rolle, aber nicht alleine. Ich muss natürlich diese Prozessoren auch entsprechend beschäftigen können. Das heißt, ich brauche eine gewisse Parallelität in meiner Anwendung. Aber zusammenfassend kann man leicht sagen, dass neben den technologischen Verbesserungen die Parallelisierung natürlich die wichtigste Rolle spielt. Und zwar erstmal die Parallelisierung innerhalb des Mikroprozessors, die komplizierte Parallelisierung. Das heißt, einerseits kann ich natürlich den Wortparallelismus steigern. Wir haben am Anfang 8-Bit-Systeme gehabt, dann 16-Bit, dann 32-Bit. Heutzutage ist Standard ein 64-Bit-System. Man kann sich natürlich auch 128-Bit oder 256-Bit-Systeme vorstellen. Allerdings muss das Ganze natürlich in Hardware gegossen werden. Das heißt, ich brauche dann entsprechend breite Busse und breite Register. Und da gibt es sicherlich irgendwo ein Optimum, das es eben gilt als Rechnerarchitekt zum Beispiel herauszufinden. Ist 64-Bit schon optimal oder ist 128-Bit vielleicht noch besser als 64-Bit? Pipelining, davon spüre ich normalerweise als Programmierer nichts. Das macht das System, das ist implizit. Das heißt, die Hardware pipelined meine Instruktionen und als Programmierer merke ich davon nichts. Aber dieses Pipelining, da gibt es ja eine Entwicklung. Man hat begonnen mit einer einfachen Pipeline, sagen wir mal 5-Stufen. Inzwischen gibt es Pipelines, die bis zu 20-Stufen haben. Das hat alles Vor- und Nachteile. Und da muss man auch wieder diesen Trade-off finden. Was ist die beste Pipeline-Länge für einen bestimmten Zweck? Ist es jetzt eine 5-Stufige oder ist es eine 20-Stufige? Das hängt von der Realisierung ab. Super-Skalarität, davon merke ich auch nichts als Programmierer, das ist auch Prozessor-intern. Das bezieht sich darauf, wie viele Befehle ich gleichzeitig bearbeiten kann. Und zwar nicht im Pipelining-Modus, sondern nebenläufig. Das heißt, ich starte die Bearbeitung von 4 aufeinanderfolgenden Befehlen gleichzeitig. Das geht natürlich nur, wenn keine Abhängigkeiten bestehen. Man muss schauen, wie viel Potenzial die Super-Skalarität hat. Ist 4-fach Super-Skalar das Beste oder ist 8-fach Super-Skalar möglicherweise besser? Auch hier gibt es wieder den Trade-off zwischen Effizienz und Leistung. Gut, Multiprozessoren, klar. Wir schließen viele Monoprozessoren zu einem großen System zusammen. Was ich aber natürlich auch machen kann oder was gemacht wird heutzutage, Multi-Cores. Das heißt, ich habe einen Prozessor-Chip und der ist schon als Multiprozessor aufgebaut. Ich habe also mehrere Kerne, heutzutage 4 bis 6 in der Regel oder 8 oder auch noch mehr, die dann eben gemeinsam ein Problem bearbeiten. Ja, wir haben vorhin anhand der Taktrate schon mal ein bisschen so eine S-Kurve gesehen. Okay, ich wiederhole es gerne mal. Ich bekomme natürlich Fragen, wie man ins Ohr geflüstert. Kann man denn durch eine Verdoppelung der Hardware automatisch eine Verdoppelung der Leistung erzielen? Das ist eine gute Frage. In der Theorie ja. Also man hat natürlich, wenn man die Hardware verdoppelt, erstmal die doppelte Leistung zur Verfügung. Ob man diese Leistung auf die Straße bringt, das heißt, ich muss natürlich auch Probleme finden, die sich mit der verdoppelten Hardware in der doppelten Geschwindigkeit lösen lassen. Warum funktioniert das in der Regel nicht? Weil ich kann ein Problem nicht so einfach voll parallelisieren. Ja, dann wäre es embarrassingly parallel. Ich habe Datenabhängigkeiten, Synchronisationspunkte, Kommunikationspunkte und die Kostenleistung. In der Tabelle sieht es nach doppelter Leistung aus, aber auf der Straße sieht es nach deutlich weniger Leistung aus, weil eben das Problem, das mit dieser Hardware gelöst wird, sich nicht zu 100% parallelisieren lässt. Also das ist vielleicht die etwas allgemeine Antwort drauf, aber die detaillierte Antwort, die sehen wir dann noch im Laufe dieser Vorlesung, wenn wir zum Beispiel über Superskalarität sprechen oder eben über die Multiprozessoren sprechen. Gut, wir waren gerade bei dieser S-Kurve, die wir ganz gut sehen. Also ich hoffe, die Frage ist beantwortet. Falls die Frage nicht ausreichend beantwortet ist, fragen Sie bitte nochmal nach. Ich kriege das dann schon ins Ohr geflüstert und versuche natürlich dann darauf einzugehen. Ich freue mich immer, wenn Fragen kommen ihrerseits. So, bei der S-Kurve war es jetzt so, dass ab 3-4 Gigahertz ist es da nicht weitergegangen. Was beschreibt diese S-Kurve jetzt allgemein? Wir haben hier den Aufwand, den wir treiben müssen und haben hier den Ertrag, den wir eben erzielen. Und man sieht, wenn man eine neue Technologie entwickelt, dann hat man am Anfang bei der Einführung dieser neuen Technologie sehr hohe Kosten, Entwicklungskosten zum Beispiel. Also sehr hohe Anstrengungen. Dann hat man irgendwann diese Technologie im Griff. Also zum Beispiel Hochintegration, also faules E-Chips. Und wenn man dann diese Technologie mal im Griff hat, dann feilt man da, macht man so kleine Veränderungen, die jeweils zu einem deutlich besseren Verhalten führen, also die zu sehr viel Benefit führen. Das heißt, dieser Teil der Kurve ist eigentlich der interessante. Ich stecke wenig Aufwand rein und erreiche viel Benefit, also viel Ertrag dadurch. Und irgendwann ist aber diese Technologie erschöpft und geht in diesen Saturation-Bereich und dann ist es natürlich nicht so gut, die weiter zu betreiben. Das heißt, dann müsste ich jetzt wieder hier anfangen, eine neue Technologie zu entwickeln, die dann irgendwann zu noch mehr Leistung führt. Also wenn ich hier in diesem Bereich bin, dann kann ich noch so viel Effort reinstecken. Ich werde einfach nicht mehr leistungsstärker Und dieses Verhalten, das wird als S-Kurve bezeichnet, ist also das Verhältnis von Aufwand zu Ertrag und ist in der Rechenarchitektur so ein typisches Verhalten, aber nicht nur da. Das kann man auch für andere Technologien verwenden. Ja, ganz interessant finde ich diese Grafik hier. Da geht es um Hype Cycle for Emerging Technologies aus dem Jahr 2015 und ist von Gartner veröffentlicht. Also der Gartner Hype Cycle. Deshalb nochmal zur Übersetzung der Legende unten. Also Truff heißt senke und Slope heißt schräge und wir sehen hier so eine typische Kurve für Technologien, die entweder gerade am Horizont sind oder die sich teilweise schon im Produktivbereich finden. Wie funktioniert das Ganze überhaupt? Ganz am Anfang steht immer der sogenannte Innovation Trigger. Also ich habe eine innovative, tolle Idee und die möchte ich ganz gerne realisieren. Um die realisieren zu können, brauche ich Geld. Das heißt, was mache ich? Ich wecke erst einmal Erwartungen bis zum Peak of Inflated Expectations. Also bis zu einem Peak, wo dann jeder denkt, da müssen wir Geld investieren, das löst alle Probleme dieser Welt. Also man schaukelt sich da ein bisschen hoch auf dieser Kurve und wenn es dann geht, diese Technik tatsächlich zu realisieren, dann fällt man erst einmal wieder in den Tal, nämlich in die Truff of Disillusionment. Das heißt, man wird desillusionisiert. Also einem werden alle Illusionen genommen und man denkt, okay, so toll ist das Ganze doch nicht. Und man muss aber durch dieses Tal durch, man muss einfach weiterentwickeln und irgendwann, wenn man Glück hat, kommt man in dieses Slope of Enlightenment. Also man erfährt ein bisschen Erleichterung, weil es funktioniert doch so einiges und wenn sich das dann bewährt, dann wird man irgendwann das, was man möchte, nämlich das Plateau of Productivity erreichen. Und im Jahr 2015 hat man mal versucht, diese Technologien, die da so am Horizont erscheinen, ein bisschen auf dieser Kurve einzusortieren und die Legende auf dieser Kurve heißt, wenn es so ein weißer Kringel ist, dann ist man in weniger als zwei Jahren soweit, so ein hellblauer Kringel in zwei bis fünf Jahren, so ein roter Kringel in fünf bis zehn Jahren, so ein gelbes Dreieck, mehr als zehn Jahre und so ein roter Punkt, den wir hier zum Glück nicht sehen, das ist obsolet bevor Plateau, das heißt, das wird nie funktionieren und es kann sich auch erst relativ spät herausstellen, dass irgendeine Technologie nicht funktioniert, dann hat man halt sehr viel Geld versenkt. Und man sieht hier, dass man im Jahr 2015 z.B. gesagt hat, dass der nächste 3D-Printing, das ist schon auf der Slope of Enlightenment, das heißt, da ist man davon ausgegangen, dass in den nächsten fünf Jahren ein ganzes Land, das ist ein ganzes Land, das in den nächsten fünf Jahren ein ganzes Land, ein ganzes Land, ein ganzes Land,

Jahren und es ist ja auch tatsächlich geschehen, dass da oder in den nächsten zwei bis fünf Jahren, dass das auf jeden Fall produktiv eingesetzt wird und 3D Drucken im Industriebereich, das wird produktiv eingesetzt. Also das funktioniert alles schon. Die Gesture Control, ja auch das, also die Gestenkontrolle, auch das ist heutzutage kein Problem mehr, wird auch eingesetzt. Virtual Reality, ja wird auch immer mehr eingesetzt. Dann Fahrzeuge, die autonom fahren und zwar als Versuchsfahrzeuge, ja das funktioniert schon ganz gut. Während Fahrzeuge, die allgemein autonom fahren, ja die sind noch ein bisschen auf dem Peak of Inflated Expectations. Das heißt, man erwartet sich sehr sehr viel von diesen autonomen Fahrzeugen. Wenn man dann aber mal ins Detail schaut, welche Probleme da noch zu lösen sind, bevor ein Fahrzeug wirklich komplett autonom durch die Gegend guckt, dann wird es schon noch ein bisschen dauern und die Frage ist, welches Niveau erreicht man überhaupt. Also man kann sich heutzutage zum Beispiel vorstellen, dass man sein Auto unten am Parkhaus abgibt und dieses Auto dann selbstfahrend in einen Parkplatz fährt und dann auch selbstfahrend aus dem Autohaus wieder rausfährt, weil so eine Garage oder so ein Parkhaus, es ist eine kontrollierte Umgebung, die kann ich schön beschildern, da kann ich schön Sensoren, Aktoren reinbauen, das funktioniert alles. Aber dass ein Auto komplett autonom über die Autobahn, durch irgendwelche Baustellen durchfährt, dahin wird sicherlich noch ein bisschen dauern. Es wird zwar wahnsinnig viel Entwicklungszeit reingesteckt und sehr viel Geld reingesteckt, das ist natürlich ein toller Marketing-Gag, wenn ich sagen kann, das Auto fährt vollkommen autonom, aber ich bin da eher skeptisch, dass das in den nächsten fünf bis zehn Jahren tatsächlich so funktionieren wird. Natürlich kann ein Auto heutzutage auf der Autobahn ein paar hundert Kilometer autonom fahren, aber wie das Beispiel Tesla eben zeigt, ist es noch nicht so ganz 100 Prozent. Wenn ein großer weißer LKW mit dem Schatten verwechselt wird oder mit Blenden durch die Sonne verwechselt wird, dann geht es halt tödlich aus und von daher gesehen würde ich momentan mich noch nicht Zeitung lesend oder filmguckend in ein Tesla- Fahrzeug setzen, sondern würde da schön die Hände am Lenkrad behalten. Und so kann man sich aber hier diese ganze Skala mal anschauen, über was sich die Leute alles Gedanken gemacht haben, was so Emerging Technologies sind, finde ich insgesamt ganz interessant. Gut, was sind jetzt für Monoprozessoren unsere Beschränkungen, also was bremst momentan die Leistungsentwicklung? Ja, die Gesetze der Physik, also Speed of Light zum Beispiel, Lichtgeschwindigkeit, wir haben 300.000 Kilometer pro Sekunde grob und das heißt, wenn ich so ein Delta-T von 0,1 Nanosekunden habe, dann kann sich mein Signal maximal drei Zentimeter fortbewegen. Das ist so eine Größe, die auf dem Jit wichtig ist und wenn ich jetzt an Verbindungsnetzwerke denke, dass ich also einen Multiprozessor habe, wo Racks miteinander kommunizieren wollen, dann ist natürlich so ein 10 Nanosekunden, kann sich mein Signal zu drei Meter bewegen, dann ist das auch wieder so eine Grenze. Ich muss ja das Signal von einem Rack ins nächste schicken, das heißt für die Verbindungsnetzwerke gilt natürlich auch die Lichtgeschwindigkeit. Die Kosten sind natürlich auch ein Problem, das heißt die Entwicklung eines neuen Mikroprozessors verschlägt eine Menge Geld. Wenn ich also so ein neues Halbleiterwerk aufbaue, sind wir bei drei bis vier Milliarden Dollar und das muss sich natürlich irgendwann mal wieder reinspielen in Entwicklungskosten. Das heißt, ich brauche die entsprechende Stückzahl an Chips, um diese Entwicklungskosten wieder reinzuspielen. Ansonsten würde der Chip viel zu teuer werden. Die Pipeline-Tiefe ist limitiert, das hat man ganz klar gesehen. Ich glaube, da hat man die Grenzen ausgetestet und eben festgestellt, wenn ich eine 14 oder 20-stufige Pipeline habe, dann ist es gar nicht so einfach, die ausreichend schnell mit Daten zu füttern. Da wird man also nicht sehr viel weiter noch gehen und es ist natürlich klar, ich nutze auf allen Ebenen die Parallelität. Wenn ich jetzt diese Parallelität nutze durch Pipeline und Superskalarität in den Monoprozessoren, dann habe ich natürlich auf höherer Ebene, auf Instruktionsebene, eben eine geringere Parallelität, weil wenn mehrere Instruktionen gleichzeitig bearbeitet werden, dann kommt es natürlich zu Konflikten und dieser Konfliktabstand ist einfach kleiner. Wenn ich, sagen wir mal, alle zehn Instruktionen einen Konflikt habe, dann habe ich eben zehn Zyklen Zeit, mir Gedanken über diesen Konflikt zu machen, den FN-Will zu lösen. Während, wenn ich jetzt eine fünffach Superskalare Maschine habe, die immer fünf Befehle gleichzeitig bearbeitet, dann habe ich ja jeden zweiten Zyklus einen Konflikt. Entschuldigung, die Stimme wird trocken, muss man ab und zu was trinken. Okay, gut, also eine der Fragestellungen für den Rechenarchitekten ist jetzt, wo führt ein zusätzlicher Aufwand zum besten Ertrag und es stellt sich eben heraus, dass es am erfolgsversprechendsten ist, wenn ich mehrere Technologien, die alle eben möglichst in diesem steilen Bereich sind, also die sich noch nicht in der Saturierung befinden, dass das der beste Weg ist. Was kann ich machen? Ich kann Komplexität reduzieren, damit ich die Prozesse besser beherrschbar mache und zwar indem ich Modularität einführe, also das ganze schön strukturiere, indem ich die Lokalität nutze und indem ich repliziere, das heißt einfach kopiere. Also Multicore Prozessoren ist natürlich das beste Beispiel. Ich nehme einen schön entwickelten Einzelprozessor und dann haue ich den vier, acht mal auf den Chip drauf. Das ist modular und das ist Replikation ausgenutzt und damit kann ich das ganze natürlich vielleicht auch besser beherrschen, das heißt letztendlich habe ich auch die Komplexität reduziert, weil ich das ganze ein bisschen aufgeteilt habe. Ich kann an Spezialisierung denken. Wir erleben es heute im Bereich der Grafikprozessoren, in dem eben General Purpose Graphical Processing Units aufgebaut werden, also GP, GPUs und die können einerseits natürlich bestimmte Aufgaben ganz besonders gut, also alles was zum Beispiel mit Grafik zu tun hat, die können aber auch normale Aufgaben noch ganz gut rechnen und es ist so eine interessante Fragestellung der zukünftigen Rechenarchitektur. Wie sieht der Rechner der Zukunft aus? Sieht er mehr aus wie so ein Multicore, das heißt wenig leistungsstarke Kerne auf einem Chip oder sieht er mehr aus wie so eine Grafikkarte, das heißt sehr sehr viele nicht besonders leistungsstarke Kerne auf einem Chip. Aber was ist der beste Weg? Das sind zwei Wege die man gehen kann. Momentan ist diese Grafikprozessoren Architektur eben hauptsächlich erfolgreich als Special Processing Unit, als Grafikprocessing Unit. Vielleicht ist das auch ein Weg wie zukünftige Rechner aussehen können für General Purpose Computing. Dann kann ich auch an den verstärkten Einsatz von FPGAs denken, also dass ich quasi Algorithmen in Hardware gieße und dass ich also eine Massenproduktion mit Spezialisierung kombiniere, rekonfigurierbare Systeme baue und mir da Leistung entsprechend generiere. Einfaches Beispiel aus alter Zeit, also im Jahr 1980 hat man sich mal Gedanken gemacht in dem Rechenbeispiel. Was ist denn die effizienteste Art um zwei Millionen Transistoren zu nutzen? Da baue ich jetzt einen superskalaren Monoprozessor, der arbeitet mit einer Frequenz von 1 Gigahertz, der hat ein Cycles per Instruction von 0,5, das heißt pro Zyklus macht er zwei Instruktionen und ich erreiche dementsprechend 2000 MIPS, also 2000 Millionen Instruktionen pro Sekunde mit dieser Architektur. Jetzt kann ich natürlich diese zwei Millionen Transistoren auch auf Multiprozessoren mit 16 Knoten verwenden, das heißt ich habe dann natürlich deutlich weniger Transistoren pro Knoten, also ungefähr 125.000, das heißt der Einzelknoten ist deutlich weniger leistungsstark, also der wird dann 0,6 MIPS pro Knoten erreichen, die Frequenz ist die gleiche, die CPI ist eben dreimal schlechter, also 1,5 und wenn ich das jetzt dann ausmultipliziere mit den 16 Knoten, dann erreiche ich eine Peakleistung von 10.000 MIPS. So und jetzt sind wir bei der Frage von vorhin, wenn ich die doppelte Hardware spendiere, bin ich dann doppelt so schnell, also in dem Fall habe ich natürlich 16 Prozessoren und bin fünfmal so schnell, selbst das ist nicht ganz die Wahrheit, ich werde keine 10.000 MIPS erreichen, weil ich bei 16 Prozessoren habe ich Kommunikationsverluste, Synchronisationsverluste, Datenabhängigkeiten und ich bin eigentlich in der Regel schon froh, wenn ich das System zu 50% auslasten kann, das heißt ich werde vielleicht real so im Mittel um die 5000 MIPS erreichen, aber, und das ist das aber, ich bin damit immer noch zweieinhalb mal so schnell als bei einem Monoprozessor. Dieses einfache Rechenbeispiel soll nur veranschaulichen, dass es sich lohnt parallel zu denken in Parallelrechner. Warum man nicht nur parallele Systeme baut, darüber werden wir im Verlauf dieser Vorlesung natürlich auch noch sprechen. Ja, hier haben wir so das Layout von so einer typischen schnellen Grafikkarte, das ist eine NVIDIA CUDA 5 Game Engine, das ist der Kepler-Multiprozessor und wir haben hier 192 Single Precision CUDA Cores, 64 Double Precision Einheiten, 32 Special Function Units, also wie es da unten beschrieben steht, also massiv parallele Systeme und was ist jetzt das Problem? Also die Leistung ist da, da muss ich einfach alles ausmultiplizieren, dann komme ich auf eine sehr hohe Leistung, aber wenn ich so voll parallel denke, als Multiprozessor denke, dann habe ich natürlich so ein System deutlich schwieriger zu programmieren. Warum? Ich habe diesen Designparameter Topologie, den ich bei sequentieller Programmierung überhaupt nicht brauche, aber jetzt muss ich mir Gedanken machen, wo platziere ich wann welche Daten und Befehle? Ja, weil ich muss eben dieses Problem der Datenabhängigkeiten so gering wie möglich halten, das heißt so gut wie möglich lösen und es ist ein sehr komplexes Problem und man kann das natürlich versuchen dem System zu überlassen, einem automatischen Parallelisierer, aber oft ist Userwissen von Vorteil und ich kann als User manuell bestimmte Sachen deutlich besser parallelisieren, als ich das automatisch kann, aber dafür brauche ich natürlich einen Programmierer, der parallel denken kann und das ist nicht so ganz trivial. Ich bin natürlich abhängig von der Anwendung, wie gut ich das System nutzen kann. Ich weiß, dass ich, wenn ich eine Auslastung von 50 Prozent erreiche von dieser Grafik hatte, dann bin ich schon gut und ich muss meine Algorithmen ändern, um überhaupt eine gute Effizienz zu erreichen. Also die Programmierung, die ist schwierig und sehr gewöhnungsbedürftig, da brauche ich viele Übungen, da brauche ich richtig gute Leute. Noch schwieriger ist es natürlich, wenn ich so einen Fehler eingebaut habe in mein paralleles Programm und ich muss dann verteiltes Debugging machen. Ja, der Fehler kann ja irgendwo auftreten. Wir haben hier einen Haufen von Prozessoren, das wollte ich eigentlich, wir haben hier einen Haufen von Prozessoren und überall hier kann ein Fehler sein und so einfach wie im sequentiellen Programmieren, wo ich einfach einen Breakpoint setze, mir dann meine Daten anschaue, so einfach funktioniert das Debugging im parallelen Fall eben nicht. Außerdem ist es oft so, dass bei diesen Systemen, weil es eine relativ neue Technologie ist, die Entwicklungsumgebung nicht so toll ist, das heißt Compiler, Lastverteiler, Optimierer, ist alles noch ein bisschen rudimentär und eines der wichtigsten Probleme, ich kann nicht so einfach existente Software draufschmeißen. Das heißt, die legacy Software ist nicht per se portable. Möglicherweise kann ich sie automatisch portable machen, aber dann ist sie sicherlich immer noch nicht effizient. Das heißt, ich kann nicht einfach neu übersetzen und die laufen lassen. Kann ich schon, aber unter Umständen bin ich dann enttäuscht, was die Leistung betrifft. So, gut, wollen wir uns das Ganze noch mal zusammenfassen anschauen, beziehungsweise noch mal genauer anschauen, wo bestehen jetzt überall Sättigungstendenzen und wenn wir uns jetzt die Silizium-Technologie anschauen, die heutzutage verwendet wird, dann sehen wir da eine Sättigung am Horizont. Wir haben hier unsere Zeitleiste, also 1971 haben wir noch mit zehn Mikrometer gefertigt unsere Transistoren. Im Jahr 2021 wird möglicherweise die 5-Nanometer-Technologie eingeführt. Momentan ist man irgendwas bei zwischen 7 und 10 Nanometer, die man verwendet. Warum kann ich jetzt nicht viel kleiner werden? Ist klar, Leiterbahnen zur Verbindung zweier Komponenten, paar Atome müssen da schon noch drüber fließen. Das heißt, eine gewisse Mindestbreite ist da erforderlich und andererseits ist natürlich, je kleiner ich werde, desto mehr Energie produziere ich auf der gleichen Fläche und das führt natürlich relativ zügig dazu, dass ich nicht mehr den ganzen Chip mit voller Leistung betreiben kann. Wir haben hier eine Skala von 45 Nanometer-Technologie bis 8 Nanometer-Technologie und hier ist aufgezeigt der Anteil von Dark Silikon. Dark Silikon ist der rote Anteil hier und was ist Dark Silikon im Unterschied zu Active Silikon? Dark Silikon ist der Anteil an Silizium, der aufgrund der thermischen Bedingungen oder thermischen Einschränkungen eben nicht mit Volllast betrieben werden kann und das führt dazu, dass wenn ich eben so eine 8-Nanometer-Technologie verwende, dass 50 bis 80 Prozent des Silikons Dark Silikon sein können. Dass ich also bei mit voller Last nur 50 oder weniger Prozent meiner Chipfläche nutzen kann. Deshalb macht möglicherweise eine weitere Verkleinerung der Fertigungsstrukturen keinen Sinn. Was ist denn wichtiger in Zukunft? Zum Beispiel die Leistungsaufnahme bei mobilen Geräten. Wir haben vorhin darüber gesprochen, es wäre gut, wenn so ein Handy mal länger als einen Tag wieder mit Batterie auskommt. Auch bei der Prozessorarchitektur, es gibt so einige Sättigungstendenzen, haben wir vorhin darüber gesprochen, also Pipeline noch tiefer macht möglicherweise keinen Sinn. Ist es sinnvoller mehr als 64 Bit breit zu werden? Das ist auch eine große Frage, also 128 bis 256 Bit, wie viel bringt mir das tatsächlich? Und 512 Bit, das ist dann echt die Frage, ob sich das noch lohnt. Das heißt, ich muss mir dann überlegen, gibt es denn alternative Architekturen zu dem Modell, wie ich es heutzutage kenne, zum Beispiel Quantencomputing. Und muss ich denn meine Optimierungsziele überdenken? Das heißt, Leistung, also Flops oder Mips ist nicht mehr alles. Leistung pro Quadratzentimeter Chipfläche ist möglicherweise ein neues Optimierungsziel. Gut, als Rechnerarchitekt schaue ich mir also alle Entwicklungsoptionen an, die ich habe, mache eine Analyse und versuche dann die irgendwie so S-Kurve mäßig einzuordnen. Das heißt, wo bringt Aufwand noch viel und wo bringt viel Aufwand nicht mehr viel. Ich schaue also auf die wichtigsten Größen erstmal, was kann ich am Takt noch machen, was kann ich an der CPI, seitens der Instruction noch machen, wie weit kann ich die Superskalarität treiben. Ich schaue mir den Instruktionslevelparallelismus an, also wie viel Parallelität steckt denn in der Abfolge von ein paar Instruktionen, wo sind die Grenzen, dass sich das eben lohnt. Das hat natürlich direkten Bezug zur Superskalarität. Dann kann ich mir eben die alternativen Ziele überlegen als Entwicklungsoptionen und versuchen neue Systemlevelvisionen aufzubauen. Gut, also hier noch mal das Beispiel mit der Pipeline. Wir haben eine sehr einfache Pipeline in Grundlage der Rechnerarchitektur zum Beispiel eingeführt. Die hat fünf Phasen und diese fünf Phasen sind Instruction Fetch, Decode, Execute, Speicherzugriff, Mem und Write Back. Das heißt, eine normale Instruktion dauert fünf Zyklen, weil diese fünf Phasen durchlaufen werden müssen und wenn ich eben diese Instruktionen, viele Instruktionen durch die Pipeline durchjage, dann wird sich die Pipeline lohnen und es heißt die Instruktionslatenz ist eben Cycles per Instruction mal Taktzyklus und bei einer optimal gefüllten Pipeline, wenn ich fünf Stufen habe, kann ich dann auch tatsächlich fünfmal so schnell werden. Die ideale Performance ist natürlich eins durch diese Instruktionslatenz und das kann ich dann durch Einsätzen vereinfachen zu Taktfrequenz durch den CPI Ideal. Der CPI Ideal bei dieser Pipeline ist natürlich eins, dass ich also im Mittel eine Instruktion pro Einheit abarbeite und die reale Performance, das ist die, die ich tatsächlich erreiche, also die ideale, das ist die, die im Verkaufsprospekt steht und die reale, das ist die, die ich mit meinen Anwendungen erreiche, die errechnet sich aus Taktfrequenz durch den idealen CPI plus die CPI Delays, die eben herrühren von Verlusten durch Kommunikation und eben bedingt durch Datenabhängigkeit. So, schauen wir uns hier nochmal an, rein die Leistungsverbesserung durch die Taktrate, also die Taktrate nach oben bedeutet natürlich, dass die Leistung nach oben geht und das hat ganz gut funktioniert von 1970 bis 2011, exponentielles Wachstum bis eben irgendwann mal, also das hier, was hier aufgezeigt wird, ist, was Intel vorhergesagt hat und Intel hat vorhergesagt, dass wir 2008, 2009 bei 10 Gigahertz sind. Die Realität hat gezeigt, dass diese Roadmap nicht ganz korrekt ist, weil wir nämlich im Jahr

2002 hier bei den 3 GHz hängen geblieben sind und seitdem geht es eben gerade rüber und dass das hier leider nicht eingetreten ist, diese Leistungssteigerung. Also hier haben wir eine ganz klare Sättigung. Es wird auf Sicht nicht möglich sein, die Frequenz noch viel weiter nach oben zu schrauben. Wie schaut es aus mit dem CPI-Ideal? Ja klar, wenn ich eine 5-stufige Pipeline habe, ist mein CPI-Ideal natürlich gleich 1. Das heißt, wenn die Pipeline eingeschwungen ist, flutscht mir nach jedem Zyklus natürlich ein Befehl raus und das bedeutet natürlich, dass je weiter weg ich vom CPI-Ideal bin, desto schlechter wird meine Performance, also die Performance nach unten und desto besser mein CPI-Ideal wird, desto besser wird die Performance. Das Ganze steht im Nenner, das heißt natürlich, wenn das CPI-Ideal größer wird, dann wird die Leistung kleiner, also das was im Nenner steht, muss natürlich kleiner werden, das heißt mein CPI-Ideal muss so klein wie möglich werden. Das erreiche ich zum Beispiel durch Superskalarität dann, dass es noch kleiner als 1 wird, also die Pipeline kann im Idealfall nur 1 erreichen, eine Instruktion, weil mehr Parallelität steckt da nicht drin. Ich kann aber natürlich mehrere Befehle gleichzeitig und alle diese Befehle wiederum sind nochmal gepipelined, kann ich gleichzeitig bearbeiten, das heißt also, dass ich alle Instruktionseinheiten vervielfache, dass ich also hier gleichzeitig die Abarbeitung von 4 Befehlen betrachte und diese 4 Befehle, die können alle gepipelined abgearbeitet werden und dann würde ich natürlich das CPI-Ideal von 1 auf 1 Viertel reduzieren, das heißt im Idealfall mache ich dieses CPI-Ideal kleiner auf 1 Viertel und dann ist die Leistung wieder deutlich besser. Wenn man allerdings Superskalar arbeitet, dann verwendet man häufig eine andere Metrik, dann sagt man nicht CPI, Cycles Per Instruction, sondern man sagt IPC, also Instructions Per Cycle, also wie viele Instruktionen schaffe ich pro Zyklus und das wäre in dem Fall eben 4 Instruktionen pro Zyklus bei dieser Architektur. So, das Problem ist natürlich, sind die Delays, das heißt, wenn ich Cache Misses zum Beispiel habe, dann führt das zu empfindlichen Leistungseinbußen, diese Cache Misses sind hier bei diesen CPI-Delays mit drin zusammengefasst, also sind irgendwelche Delays, die ich erfahre und man sieht hier unten am realen Beispiel, wie viel Fläche zum Beispiel eben geopfert wird für Caches, damit ich die möglichst groß machen kann und damit eben möglichst selten so ein Cache Miss eintritt. Das ist also ein Chipfoto vom Itanium 2 Prozessor und wir sehen hier, dass den größten Teil, fast den größten Teil der Fläche dieses Chips eben der 3 Megabyte große L3-Cache einnimmt, also wenn ich den L3-Cache On-Chip habe, dann nimmt der natürlich sehr viel von meiner Chipfläche ein, also ich kann das auch nicht beliebig hochtreiben, obwohl natürlich heutzutage noch viel, viel mehr Transistoren zur Verfügung stehen und ich natürlich deutlich mehr Cache auf so ein Chip draufpacken kann. Dann haben wir eben die Datenabhängigkeiten, die den Parallelismus auf Instruktionsebene einschränken, das heißt jeder Konflikt, der zu Stalls oder No-Ops führt, der kostet mich Performance und das Ganze steckt wieder in den CPI-Delays drin, gehen nach oben und dementsprechend geht die Leistung nach unten. Dann haben wir mit einer Pipeline noch die andere Art von Konflikten, also nicht nur Datenabhängigkeiten, sondern auch Kontrollkonflikte, Kontrollkonflikte entstehen insbesondere bei Sprüngen, bedingten Sprüngen, unbedingten Sprüngen oder bei Aufruf von Unterprogrammen, wobei ein Unterprogrammaufruf programmtechnisch auch nichts anderes darstellt als ein Sprung. Immer da habe ich Probleme, führt also zu mehr Delays und dementsprechend zu schlechterer Leistung. So, wenn wir uns jetzt mal so einen superskalaren Prozessor anschauen, also einen 4-fachskalaren Prozessor, der dann auch noch jeweils eine 5-stufige Pipeline hat für jeden einzelnen Befehl, dann habe ich also, oder wenn ich eine größere Pipeline betrachte, hier in dem Beispiel jetzt eine 20-stufige Pipeline betrachte, wie es sie auch schon gegeben hat in aktuell existierenden Prozessoren, dann habe ich also 4 mal 20, habe ich 80 Instruktionen gleichzeitig in Bearbeitung und das ist natürlich eine sehr hohe Konfliktwahrscheinlichkeit, weil ich so ein Instruction Window von 80 habe und da werden sich sicherlich die eine oder andere Datenabhängigkeit drin befinden, die ich dann natürlich lösen muss und die dann eben dazu führt, dass diese 4-fachskalarität nicht vollständig ausgenutzt werden kann und dementsprechend nicht zur ganz gewünschten Leistungssteigerung führt. Das sind alles Sachen, da muss ich viele Simulationen laufen lassen, viele Tests laufen lassen, analytische Untersuchungen vom Programmcode machen, um dann zu entscheiden als Rechenarchitekt, ist es denn besser eine 10-stufige oder eine 8-stufige Pipeline zu realisieren? Ist es denn besser 2-fachskalar oder 4-fachskalar zu arbeiten? Also es spannt sich immer ein sehr großer Entwicklungsraum auf und ich muss da die möglichst guten Punkte drin finden als Rechenarchitekt und die entsprechend realisieren. Gut, was kann ich jetzt bei den Sprüngen zum Beispiel machen? Wir werden das im Detail in dieser Vorlesung sehr sorgfältig betrachten, hier nur so ganz allgemein erstmal. Ziel ist natürlich den Prozessor zu beschäftigen. Und wenn ich jetzt Kontrollkonflikte durchsprünge habe, dann habe ich zwei Möglichkeiten. Ich kann versuchen das Sprungziel vorher zu sagen, wenn ich zum Beispiel eine Laufschleife habe, dann wird jedes Mal zum gleichen Ziel wieder gesprungen bis die Laufschleife zu Ende ist. Ich merke mir also einfach wohin ich gesprungen bin, nutze dieses Wissen aus, um dann Parallelität reinzubringen. Eine ganz andere Möglichkeit, die ich habe, ich mache einfach eine Spekulation, das heißt ich berechne, was passiert, wenn der Sprung genommen wird und berechne, was passiert, wenn der Sprung nicht genommen wird. Irgendwann weiß ich, ob der Sprung genommen wird oder nicht genommen wird und dann gehe ich auf dem Pfad weiter und das andere, was ich parallel dazu berechnet habe, schmeiße ich einfach weg. Das heißt aber, wenn ich so arbeite, möglicherweise habe ich dann eine sehr gute Leistung, aber ich habe natürlich sehr viele Instruktionen umsonst gerechnet, die ich einfach wegschmeiße und das ist natürlich verschwendete Energie und das ist die Frage, kann ich mir verschwendete Energie leisten. Bei einem Supercomputer möglicherweise ja, da stelle ich halt ein kleines Kraftwerk daneben, um den zu betreiben. Bei einem Handy möglicherweise nein, bringt mir das nicht viel, dass ich einfach da Energie verschwende. Ja, das ist jetzt wieder natürlich die ganz große Frage, wenn ich an Pipelining und Superskalarität denke, jetzt in normalen Anwendungen, wie groß ist denn eigentlich mein Parallelismus? Das heißt, wir gehen jetzt mal davon aus, wir hätten einen perfekten Prozessor, der macht eine perfekte Sprungvorhersage und der hat beliebig viele Ausführungseinheiten und ich mache da simulierte Benchmarks drauf und dann schaue ich mir mal an, wie viel Parallelismus steckt drin. Sie sehen hier in der Grafik, das ist natürlich sehr anwendungsabhängig, deshalb sehen Sie hier in der Grafik unterschiedlichste Benchmark-Anwendungen, also von Tomcat V bis zum GCC, das sind also Anwendungen mit unterschiedlicher Natur, manche sind einfache Berechnungen, manches ist einfach ein Compiler, also mehr Text-Processing, je nachdem und für diese unterschiedlichsten Anwendungen hat man das Ganze mal gemessen für unterschiedliche Window-Sizes, Instruction Window-Sizes. Was ist das? Das sind die Anzahl von Befehlen, die out of order ausgeführt werden können, also das ist das Instruction Window ohne Datenabhängigkeit. Entschuldigung, falsche Taste, ich muss nämlich Control und die linke Maustaste drücken und wenn ich die linke Maustaste zu schnell drücke, dann bin ich auf der nächsten Folie, aber ich werde mich da sicherlich noch dran gewöhnen. Ok, was sehen wir hier? Also wir sehen hier, dass selbst wenn ich 256 Window-Sizes habe, das heißt 256 Instruktionen betrachte, selbst dann kann ich hier maximal 10 Instruktionen bis 15 Instruktionen gleichzeitig ausführen. Das heißt bei vielen dieser Anwendungen ist die Parallelität auf ungefähr 15 beschränkt. Das heißt es bringt nicht viel mehr als 15 Instruktionen gleichzeitig zu bearbeiten, weil ich ansonsten Datenkonflikte habe, die ich lösen muss. Es gibt natürlich sogenannte Embarrassingly Parallel-Anwendungen, das heißt Anwendungen, da wird wenig kommuniziert, da gibt es wenig Datenabhängigkeiten, also das Mandelprozess ist zum Beispiel so eine Anwendung, da wird einfach über die Koordinate jedes Pixel ausgerechnet, da habe ich keine Kommunikation mit benachbarten Pixeln und dementsprechend Embarrassingly Parallel sind die und selbst für diese Anwendungen finde ich nur auf Instruktionsebene nur eine Parallelität von 50 bis 55. Das heißt der Parallelismus auf Instruktionsebene ist auf ungefähr 9 bis 55 Instruktionen pro Zyklus beschränkt. Das sind natürlich harte Einschränkungen für die Architektur, das heißt schlicht und ergreifend es bringt nichts Superskalarität und Pipelining zu weit zu treiben. Ja, was kann ich dann machen? Alternatives Ziel im Zusammenhang mit Multicores natürlich hochaktuell ist, ich betreibe Multithreading. Wenn ich nämlich zum Beispiel einen Server habe, der sehr viele unterschiedliche Prozesse am Start hat, die alle gleichzeitig laufen sollen, dann kann ich natürlich mir Gedanken über sowas wie Multithreading machen. Multithreading heißt, ich habe leichtgewichtige Prozesse und diese leichtgewichtigen Prozesse teilen sich den selben Speicherkontext, das heißt der Kontext switcht, ja wir erinnern uns an Grundlagen Rechenarchitektur, haben wir kurz darüber gesprochen, so ein Prozesswechsel ist natürlich immer relativ aufwendig, weil ich viele Sachen retten muss und dann, wenn ich dann wieder zurück wechsle, müssen die wieder zurückgeladen werden, die ganzen Register, Speicherkontext, alles muss gerettet werden und bei leichtgewichtigen Prozessoren entfällt das und das Problem, was ich hier aber habe, ich kann es erst so richtig nutzen, wenn ich auch eine Hardware habe, die eben diesen schnellen Kontextswitch entsprechend unterstützt. Das heißt, hier müssen Rechenarchitekten praktisch ein bisschen zusammenarbeiten mit den Betriebssystemleuten und die haben jetzt eine neue Idee, sagen, warum arbeiten wir nicht mit Threads, wenn wir mit Threads arbeiten, dann bräuchten wir aber von euch Rechnerentwicklung, Hardwareunterstützung, um so einen schnellen Kontextswitch zu ermöglichen. Gut, andere alternative Ziele, schauen wir uns mal die Situation im Jahr 1998 an. Wir haben eine Fertigungsbreite von 0,3 Mikrometern, das heißt, so ein Mikroprozessor aus dem Jahr 1998 ist der Pentium III und der bestand aus 16 Millionen Transistoren. So, schon 10 Jahre später, das heißt im Jahr 2008, habe ich eine Fertigungsbreite von 50 Nanosekunden, von 50 Nanometern Fertigungsbreite, nicht Sekunden und das bedeutet schlicht und ergreifend, dass ich auf die Chipfläche 800 Millionen Transistoren aufbringen kann. 800 Millionen zu 16 Millionen, das ist also 50 mal so viel und ich könnte also auf der gleichen Fläche im Jahr 2008 50 Mikroprozessoren aus dem Jahr 1998 unterbringen. Wenn ich mir die Integrationsdichte beim Speicher anschaue, dann ist es ähnlich, ich kann also auf der gleichen Fläche statt 80 Megabit, kann ich 4 Gigabit unterbringen. Das ist also die Fertigungsdichte im Jahr 2008. So, und jetzt kommt die interessante Fragestellung für uns als Rechenarchitekten, was mache ich denn mit dieser Chipfläche? Was baue ich denn? Ich könnte mir also bauen, zum Beispiel ein System wie hier, das besteht aus einem Mikroprozessor und 4 Gigabyte DRAM. Habe also einen sehr, sehr großen Cache und habe dadurch meinen Memory-Delay sehr niedrig gemacht wahrscheinlich. Oder ich kann auf diese Fläche, wo früher eben ein Transistor drauf, ein Mikroprozessor drauf gepasst hat, kann ich jetzt 50 Mikroprozessoren aufbringen. Das heißt, ich baue ein Multicore mit 50 Processing-Elements. Das ist das andere Ende meines Entwicklungsraums, den ich da habe und die Wahrheit wird irgendwo hier in der Mitte liegen. Das heißt, ich werde mehrere Processing-Elements aufbringen und werde eine gewisse Fläche für den Speicher opfern. Aber wo genau jetzt das Optimum ist? 2 Processing-Elements, 4, 8, das ist die Fragestellung, die wir lösen müssen und die lässt sich nicht so einfach lösen, sondern da muss ich sehr genau in alle Anwendungen schauen, da muss ich Simulationen machen, muss ich Modellierungen machen und dann komme ich eben immer zu lokalen Optima, die ich dann in Hardware realisieren kann. Gut, was auch immer wieder als alternatives Ziel oder neue Metrik genannt wird, ist die elektrische Leistungsdichte. Also, wie viel Watt pro Quadratzentimeter produziere ich denn? Hier haben wir eine Zeitskala beginnend bei 1985 und endend irgendwo 2010, 2020, weiß ich jetzt nicht genau. Egal, wir haben hier wieder exponentielle Skala, Watt pro Quadratzentimeter und so während so ein Pentium 4 noch so bei 35 Watt Wärmeproduktion lag, liegen neuere, also i-Serie zum Beispiel von Intel bei 75 Watt und wenn man das jetzt auf Watt pro Quadratzentimeter bezieht, dann ist hier so ein Punkt, so heiß kann so ein Reaktor laufen, kurz dann bevor es zum Meltdown kommt und hier unten haben wir praktisch schon so heiß wird eine Herdplatte, wenn ich die Watt pro Quadratzentimeter betrachte. Das heißt also, wir haben hier so ganz gute Heizungen am Start, zumindest auf die Fläche pro Quadratzentimeter bezogen und das ist natürlich das große Problem, wie leite ich diese große Wärme von dieser kleinen Fläche ab und das war ja auch einer der Faktoren, warum aufgrund der großen elektrischen Leistungsdichte, warum ich nicht höher takten kann. Noch ein alternatives Ziel, was ich mir stellen kann ist, warum baue ich nur einzelne Prozessoren, also Multicores, warum verwende ich jetzt inzwischen Milliarden von Transistoren, die ich auf einer Chipfläche habe, nicht um so ein ganzes System On-Chip zu bauen. Also Systems On-Chip ist ein alternatives Ziel und möglicherweise baue ich eben diese Systeme, die besonders gut sind für spezielle Anwendungen. Also zum Beispiel kann man sich vorstellen Medienprozessoren zu bauen, Netzwerkprozessoren zu bauen, die eben Networking besonders gut können, die hier, die können Streamverarbeitung zum Beispiel besonders gut. Dann Kryptografie, Kryptografie ganz wichtiges Thema, alles muss verschlüsselt sein, die Schlüssel werden immer größer. Also ist es gut, wenn ich Systeme habe, die diese Kryptografieaufgabe sehr schnell übernehmen können, Realzeit normalerweise. Signalverarbeitungssysteme, also nicht nur noch digitale Signalprozessoren, sondern ganze Systeme für die Signalverarbeitung. Das Problem, das wir hier haben, ist natürlich immer, wenn ich anwendungsspezifisch werde, reichen dann meine Stückzahl noch aus, um Entwicklungskosten zu rechtfertigen. Möglicherweise kann ein Intel i7 die Medien ein bisschen schlechter verarbeiten, aber wenn die Qualität ausreicht, dann ist es vielleicht billiger, diese Grafik zum Beispiel direkt vom Prozessor bearbeiten zu lassen, als speziell die Grafik auf eine Grafikkarte auszulagern, die dann viel, viel teurer ist. Das heißt also, ich muss einerseits die Produktionskosten irgendwie wieder einspielen und andererseits müssen diese anwendungsabhängigen Systeme natürlich deutlich schneller sein, damit man sie einem General Purpose System vorzieht. Das muss alles funktionieren. Gut, ein ganz anderes alternatives Ziel, das wurde schon ca. 1990 von Mark Weiser veröffentlicht unter dem Stichwort Ubiquitous Computing. Und was hat er beobachtet? Er hat beobachtet, ja, früher war es so, Computer waren sehr teuer, es gab nur Mainframes und ein Computer für viele Nutzer. Dann kam das Zeitalter des PCs. Ich habe meinen Personalcomputer gehabt und ich habe eben in der Regel war es ein Single-User-System, also ein Computer pro Person. Die Situation heutzutage stellt sich eben so dar, dass jeder Nutzer sehr viele Computer hat. Wir haben einen Laptop, wir haben ein Handy, inzwischen hat ja sogar unser Kühlschrank schon Internetanschluss, also ich habe sehr viele Rechner pro Person und auch dem kann man was abgewinnen, nämlich small is beautiful und das hat er als Ubiquitous Computing bezeichnet. Und hier ganz einfach, die Größe der Rechner, die wird immer weniger, wird immer kleiner, dafür steigt die Anzahl der in Betrieb befindlichen Rechner. Gut, wenn wir jetzt dieses Kapitel Revue passieren lassen, hoffe ich klar gemacht zu haben, dass die Aufgabe des Rechenarchitekten nicht einfach ist. Es spiegelt sich oft in einem Optimierungsproblem wider, das heißt ich habe einen großen Raum, in dem ich Entwicklungen machen kann, realisieren kann, aber nur einige Punkte in diesem Entwicklungsraum sind wirklich gute Punkte und es gilt eben diese Punkte zu finden. Und diese Punkte zu finden, das heißt...

Ich muss mir Gedanken machen um die Performance, ich muss mir Gedanken machen um mein Instructional Set Design, also um meinen Befehlssatz. Wie baue ich meine Alus möglichst schnell, möglichst effizient? Wie baue ich meine Pipelines? Was ist da das Optimum möglicherweise? Wie muss der Cache angebunden sein? Wie groß muss der Cache sein? Wie schnell muss der Cache sein? Wie weit kann ich Superskalarität treiben? Und was sind die Komponenten von superskalaren Prozessoren? Das sind also im Wesentlichen die Titel der Kapitel, die wir in diesem Semester besprechen, was den Monoprozessor betrifft. Und dann werden wir uns im zweiten Teil beschäftigen mit der Architektur von Parallelrechnern, werden uns da nochmal die Leistungsmodellierung anschauen und Multicore und Multithreading. Das nächste Kapitel, was Sie in StudIP finden, darauf möchte ich jetzt hier nicht groß eingehen, weil ich bin da immer wieder kritisiert worden, ja jetzt hören wir das Ganze nochmal, was ein B-Komplement ist. Dieses Kapitel T16 ist überschrieben mit Wiederholungen von Begriffen ausgrundlagender Rechnerarchitektur. Ich lege jedem ans Herz, der jetzt mit B-Komplement nichts anfangen kann und der nicht in anderen Zahlensystemen als im Dezimalsystem rechnen kann, dass er sich nochmal ganz kurz diese 16 Folien reinzieht, Wiederholung von Begriffen. Jeder, der da mal kurz reinschaut und sagt, ok, das haben wir alles schon in Grundlage digitaler Systeme und GRA gemacht, das kann ich alles, der kann dieses Kapitel natürlich gerne überspringen, ich gehe aber in der nächsten Vorlesung am Freitag davon aus, dass Sie alle Begriffe, die in diesem Kapitel auftauchen, auch verstehen und werde sie auch in der Vorlesung verwenden. Also dementsprechend, kleine Hausaufgabe, schauen Sie sich wenigstens mal kurz das nächste Kapitel T16 an. Wenn Sie alles können, umso besser, wenn nicht, ist es vielleicht eine hilfreiche Wiederholung, um dann die weitere Vorlesung zu verstehen. Gut, ansonsten würde ich sagen, früh am Morgen soll man vielleicht nicht ganz so lang sprechen, werde ich heute ein bisschen eher Schluss machen. Wenn Sie noch Fragen haben, stehe ich gern für Fragen noch zur Verfügung, entweder schriftlich oder auch mündlich und ich hoffe, wir sehen uns dann alle am nächsten Freitag um 8.15 Uhr wieder. Danke für Ihre Aufmerksamkeit.

